---
title: "How R you? - R-Ladies Melbourne 4th anniversary!"
date: "16/11/2020"
output:
  html_document:
    includes:
      after_body:
      - header.html
      - footer.html
    theme: cosmo
    toc: yes
    toc_depth: 2
  pdf_document:
    toc: yes
    toc_depth: '2'
  github_document:
    toc: yes
    toc_depth: 2
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message=FALSE,
                      warning=FALSE,
                      fig.width = 5,
                      fig.height = 5,
                      fig.align = 'center')

library(magick)
library(png)
library(grid)
library(knitr)
```


```{r echo=FALSE,message=FALSE,fig.width=3,fig.height=3,fig.align='left',cache=FALSE}
img <- readPNG("logo.png")
grid.raster(img)
```

# Using `purrr` for quick summary tables - **St Koo**


Adapted from this fantasic [Learn to Purrr tutorial](http://www.rebeccabarter.com/blog/2019-08-19_purrr/)

```{r}
# Load example data
data("mtcars")

head(mtcars)
```

You can use `purrr` to loop over all the columns, and output the info into a dataframe.

In this example, I want to see variable type, max character length, and missingness.
```{r, message = FALSE}
library(tidyverse) # Includes purrr

# Create dataframe with some summary info
summary_df <- mtcars %>%
  purrr::map_df(
    ~data.frame(
      class = class(.x),
      max_char = max(nchar(.x, keepNA = FALSE)),
      missing = sum(is.na(.x))
    ), .id ="col_name"
  )

summary_df
```

And because it's a dataframe, you can use a package like `kableExtra` to format it for reports.
```{r, message = FALSE}
library(kableExtra)

summary_df %>%
  kableExtra::kbl() %>%
  kableExtra::kable_styling()
```


# Fitting several linear models by group with `purrr` - **[Anna Quaglieri](https://github.com/annaquaglieri16)**

For the example I am going to use the `flights` dataset from the R package `nycflights13`. I am going to fit linear model that tries to explain the `arr_time` as a function of `dep_time` and `arr_delay`.

```{r}
library(nycflights13)
library(purrr)

flights %>%
  dplyr::select(arr_time, dep_time, arr_delay, carrier) %>%
  head()
```


To fit the model to the whole dataset we would use the following code:

```{r}
summary(lm(arr_time ~ dep_time + arr_delay, data = flights))
```

What if we wanted to fit separate models by `carrier`?

```{r}
models <- flights %>%
  dplyr::select(arr_time, dep_time, arr_delay, carrier) %>%
  dplyr::group_by(carrier) %>%
  tidyr::nest() %>%
  dplyr::mutate(fit = purrr::map(data, ~ lm(arr_time ~ dep_time + arr_delay, data = .))) %>%
  dplyr::mutate(results_fit = purrr::map(fit, function(f) confint(f))) 
models
```

```{r}
expand_models <- models %>%
  tidyr::unnest(results_fit, .drop=TRUE) 
expand_models
```

```{r fit-column}
expand_models$fit[1]
```


# Column-wise operations with `mutate`, `across` and `case_when` - **[Anna Quaglieri](https://github.com/annaquaglieri16)**

I found the method below really useful to recode the levels of one or several columns! 

```{r}
library(dplyr)

test_data <- data.frame(area = rep(c("North", "Sud", "East", "West"),times = c(2,3,4,1)),
                        quality_before = c("High","Low",
                                    "High","Low","Medium",
                                    "Medium","Low","High","High",
                                    "Low"),
                        quality_after = c("High","High",
                                    "High","Medium","Medium",
                                    "Low","Low","High","High",
                                    "Low"))

test_data %>%
  mutate(across(.cols = c(quality_before, quality_after),
            ~ case_when(
              . == "Low" ~ 0,
              . == "Medium" ~ 1,
              . == "High" ~ 2
            )
          )
  )
         
```


Strongly suggest to have a look at other functions and applications to perform column-wise operations https://cran.r-project.org/web/packages/dplyr/vignettes/colwise.html.


# Useful R Tips by **[Song](https://www.linkedin.com/in/ytsong/)**

## Before we start

```{r}
sessionInfo()
```


## Data Exploration

I code in **tidyverse** universe plus **tidylog** to output all message corresponding to changes made to vector, dataframe, tibble, etc. Please find tidylog package @ <https://github.com/elbersb/tidylog>.

- An initial step is often to preview a dataset. An alternative to summary is describe in **Hmisc** package. The benefits will be it counts number of NAs as summary does, but it also show a frequency table for factor/character.


```{r, message=FALSE}
library(tidyverse)

# colnames(iris)
# summary(iris)
# str(iris)

library(Hmisc)

iris$Species %>% describe
```

- **tidylog** reduces significant code verification and avoid many errors for me. It warns you when NA is generated due to situation not considered. 

```{r message=TRUE}
library(tidylog, warn.conflicts = FALSE, quietly = FALSE)

new_dt <- iris %>% 
  filter(Sepal.Length >= 4.6) %>% 
  mutate(new_name = case_when(
    Species == "versicolor" ~ "V",
    Species == "setosa" ~ "S"))
```

- Compare with revised code: no new NA was generated after the mutation.


```{r message=TRUE}
library(tidylog, warn.conflicts = FALSE, quietly = FALSE)

new_dt <- iris %>% 
  filter(Sepal.Length >= 4.6) %>% 
  mutate(new_name = case_when(
    Species == "versicolor" ~ "Versicolor",
    Species == "setosa" ~ "Setosa",
    TRUE ~ "Virginica"))
```



## Data Preparation

Use iris dataframe as an example.

- **relocate** also support **.after**, and combine use with **where**.

```{r}
dt <- head(iris,5)

# dt %>% select("Species", everything(.))
dt %>% relocate("Species", .before = "Sepal.Length")
# dt %>% relocate(where(is.numeric), .after = where(is.factor))
```

- Similar concept can be applied to a vector through **SOfun** package. I found this useful when adjusting factor levels. Of course, **fct_reorder** and **fct_relevel** are useful in different situations.

```{r}
library(devtools)
# install_github("mrdwab/SOfun", force=TRUE)
library(SOfun)
v <- letters[1:7]
v %>% moveMe(., "a last; b, e, g before d; c first; g after b")

```


- Another high frequency task is to manage NAs. This is way to spot hidden NAs.

```{r}
hidden_na_dt <- data.frame(
  "student" = rep(c("A", "B", "C"),2),
  "assignment" = rep(c("A1", "A2"),3),
  "mark" = c(NA, runif(n = 5, min = 45, max = 100))
) %>% 
  filter(!is.na(mark))

hidden_na_dt
```

- apply **complete** from **dplyr** package to fill 0 in missing mark from assignment 1 for student A. If there is more combinations, multiple items can be nesting in the **complete** argument.

```{r}
hidden_na_dt  %>% 
  complete(student, nesting(assignment), fill = list(mark = 0))
```

## Data Visualisation

I believe **ggplot2** / **plotly** is relative popular in practice. I also recommend **highercharter** to visualize timeseries data and/or **visNetwork** / **igraph** / **ggraph** to visualize networks. My focus today is labeling inside a chart, so that I will use **ggplot2** to demonstrate.

- Randomly picked a few countries by max number of population of that country to show potential difference when treating labeling.

```{r, message=FALSE}
plt_original <- population %>% 
  filter(country %in% c("India", "United States of America", "Viet Nam",
                        "Lao People's Democratic Republic")) %>% 
  ggplot(aes(x = year, y = population, group = country, color = country))+
  geom_line()

plt_original
```


### Functions that I used to improve numeric formatting

The purpose of having customized functions is to improve readability and reduce cognitive load for digesting information provided by visualization.

- The function I grabbed from stackoverflow and made two adaptations: (1) allow the function to accept negative inputs and (2) expand to recognize trillions. 

```{r, message=FALSE, warning=FALSE}
si_num <- function (x) {
  
  if (!is.na(x)) {
    
    if (x < 0){ 
      sign <-  "-"
      x <- abs(x)
    }else{
      sign <-  ""
      x <- x
    }
    
    if (x >= 1e12) { 
      chrs <- strsplit(format(x, scientific=12), split="")[[1]];
      len <- chrs[seq(1,length(chrs)-12)] %>% length();
      rem <- chrs[seq(1,length(chrs)-11)];
      rem <- append(rem, ".", after = len) %>% append("T");
    }
        
    if (x >= 1e9) { 
      chrs <- strsplit(format(x, scientific=12), split="")[[1]];
      len <- chrs[seq(1,length(chrs)-9)] %>% length();
      rem <- chrs[seq(1,length(chrs)-8)];
      rem <- append(rem, ".", after = len) %>% append("B");
    }
    
    
    else if (x >= 1e6) { 
      chrs <- strsplit(format(x, scientific=12), split="")[[1]];
      len <- chrs[seq(1,length(chrs)-6)] %>% length();
      rem <- chrs[seq(1,length(chrs)-5)]
      rem <- append(rem, ".", after = len) %>% append("M");
    }
    
    else if (x >= 1e3) { 
      chrs <- strsplit(format(x, scientific=12), split="")[[1]];
      len <- chrs[seq(1,length(chrs)-3)] %>% length();
      rem <- chrs[seq(1,length(chrs)-2)];
      rem <- append(rem, ".", after = len) %>% append("K");
    }
    
    else {
      return(x);
    }
    
    return(str_c(sign, paste(rem, sep="", collapse=""), sep = ""));
  }
  else return(NA);
} 

si_vec <- function(x) {
  sapply(x, FUN=si_num);
}
```


- Modifications include: (1) change graph title and axis titles and format, (2) change a theme: minimalist design,  (3) remove legend and add text labels to each line.
- Of course, there are more things: change color pallet defined for country, graph size,...

```{r, fig.height=5, fig.width=10,  message=FALSE, warning=FALSE}
library(hrbrthemes)
library(scales)
library(ggrepel)
library(cowplot)


year_series <- unique(population$year)
reminder <- (max(year_series) - min(year_series)) %% 4
new_breaks <- seq(from = min(year_series) + reminder, to = max(year_series), by = 4) 

df <- population %>% 
  filter(country %in% c("India", "United States of America", "Viet Nam",
                        "Lao People's Democratic Republic")) 
df_end <- df %>% 
  group_by(country) %>% 
  filter(year == max(year)) %>% 
  ungroup()

plt_adjust <- df %>% 
  ggplot(aes(x = year, y = population, group = country, color = country))+
  geom_line()+
  geom_point()+
  geom_text_repel(
    data = df_end,
    aes(label = str_wrap(country,25)),
    nudge_x = 1,
    direction = "y",## nudge vertically
    size = 3,
    hjust = 0, ### left aligned
    segment.size = 0.3, ### from here is about the line to connect the data point and text
    min.segment.length = 0,
    segment.color = "grey60") + 
  theme_ipsum() +
  theme(legend.position = "none") +
  scale_y_continuous(labels = si_vec)+
  scale_x_continuous(breaks = new_breaks, limits = c(NA, 2020))+
  labs(x = "Year", y = "Population", title = "Population Growth between 1995 and 2013")


plt_original
plt_adjust
```

- Or, put it into **plotly**, the default hover over message often does not satisfy users, more professional format is recommended to be used in hover over text.

```{r, message=FALSE, warning=FALSE}
library(plotly)

plt_plotly <- df %>% 
  mutate(text = str_c("Country: ", country, "\n",
                      "Year: ", year, "\n",
                      "Population: ", si_vec(population))) %>% 
  ggplot(aes(x = year, y = population, group = country, color = country, text = text))+
  geom_line()+
  geom_point()+
  theme_ipsum() +
  theme(legend.position = "none") +
  scale_y_continuous(labels = si_vec)+
  scale_x_continuous(breaks = new_breaks)+
  labs(x = "Year", y = "Population", title = "Population Growth between 1995 and 2013")



ggplotly({plt_plotly}, tooltip = "text")
```

### At the end

To be continue, I have coded many interactive plots in shinyapps, and some can be found from <https://coffeeandplot.com/apps/>. This is a relatively new website we created couples of month ago. Get in touch if you have any suggestions. Please find me @ <https://www.linkedin.com/in/ytsong/>.


# Render an RMarkdown report - **[Sehrish Kanwal](https://github.com/skanwal)**

This section describes how to render an RMarkdown report within a simple R conda environment on a Command Line Interface (cluster or linux environment). This could be achieved in two possible ways:

*  Creating/activating a conda environment and installing packages on commandline
*  Using an `environment.yml` file that documents the package dependencies

Both work but the second way is the recommended one, which will be described below.

1. Create an `environment.yml` file, that looks something like

	```
	#name of the conda environment
	name: HowRYou
		
	#the paths that conda takes a look for packages. Avoid using anaconda channel as we have
	#experienced issues using it 
	channels:
		- conda-forge
		- bioconda
		- defaults
		
	#install following packages in the conda environment
	#change according to the packages you are using in your RMardown file. 
	#The first three are required (are R essentail). You can also change the versions to
	# meet the requirements of your analysis 
	dependencies:
		- r-base=3.4.1
		- pandoc=1.19
		- r-rmarkdown=1.6
		- r-here
	```

2. Create a conda environment (in this case `HowRYou` is the conda environment name specified in the `environment.yml` file. `-p` flag should point to your miniconda installation path. To find how to install conda, check [this](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) 

	```
	conda env create -p /path/to/miniconda/envs/HowRYou --file environment.yml
	```

3. Activate this conda environment

	```
	conda activate HowRYou
	```
	
4. Run the RMarkdown file

	```
	Rscript -e "rmarkdown::render('HowRYou.Rmd')"
	```
	
	To pass arguments to the Rmd script (in this case two arguments - an input directory location and name of the input vcf file)
	
	```
	Rscript -e "rmarkdown::render('HowRYou.Rmd', params = list(directory = './data', file = 'dummy.txt'))"
	``` 
	
5. An [example](https://github.com/skanwal/Play/blob/master/RLadiesMelb/HowRYou.Rmd) of a rendered script used in the above step # 4

## Advantages:

- Reproducibility - ability to perform the analysis multiple times
- Portability - being able to move code from one machine or platform to another
- Flexibility - change easily in response to different user and system requirements 


# Patchwork - **[Shazia Ruybal](www.shaziaruybal.com)**

```{r}
# install.packages(c("tidyverse", "patchwork, palmerpenguins", "devtools"))
# devtools::install_github("bahlolab/ggwehi")
library(tidyverse)
library(patchwork)
# library(ggwehi)
library(palmerpenguins)
```


```{r}
plot_1 <- penguins %>%
  ggplot(aes(x = flipper_length_mm, y = body_mass_g, color = species)) +
    geom_point() +
   # scale_color_wehi() +
    labs(x = "Flipper length (mm)",
         y = "Body mass (g)",
         color = "Penguin species") +
    theme_minimal()
```

```{r}
plot_2 <- penguins %>% 
  ggplot(aes(x = flipper_length_mm, fill = species)) +
    geom_histogram(alpha = 0.5, position = "identity") +
   # scale_fill_wehi() +
    labs(x = "Flipper length (mm)",
         y = "Frequency",
         fill = "Penguin species") +
    theme_minimal()
```

```{r}
plot_1 + plot_2 + # plot them side-by-side
  plot_layout(guides = "collect", width = 8) + # this "collects" the legends to the right of the plot
  plot_annotation(tag_levels = "a", tag_suffix = ")") # this adds panel labels
```



```{r}
plot_1 / plot_2 + # plot them one over the other
  plot_layout(guides = "collect",  width = 8) + # this "collects" the legends to the right of the plot
  plot_annotation(tag_levels = "a", tag_suffix = ")") # this adds panel labels
```


# How to Generate Word Clouds in R (The simplest way) - **[Zeinab Manzari](https://github.com/XYZeinab)**

```{r}
# install.packages(c("wordcloud","RColorBrewer))
library(wordcloud) #wordcloud for generating word cloud images
library(RColorBrewer) #RColorBrewer for color palettes
```

```{r}
words <- c("RLadies", "Rmarkdown", "tips", "tricks", "script", "rmd", "console", "packages", "share", "4thanniversary", "celebrate", 
           "RSoftware", "Australia", "Melbourne", "Girls", "Learn","Teach", "Data Structure", "Algorithm", "Visualisation", 
           "Code", "Data", "ggplot2", "Zoom", "Help", "Text", "RStudio", "programing", "questions", "answers", "Plot", "happy")

freqs <- c(980, 900, 498, 811, 800, 654, 489, 90, 254, 500, 600, 200, 488, 400, 140, 250, 357, 789, 147, 120, 590, 741, 100, 788, 812, 693, 410, 753, 95, 80, 594, 644)
```

```{r}
set.seed(3)

wordcloud(words = words,freqs = freqs, scale=c(4,.4), max.words = 1000, random.order=TRUE, random.color=TRUE, colors = brewer.pal(8, "Accent"))
```


# Insert a time-stamp in RStudio - **[Adele Barugahare](https://github.com/aabarug)**

In a code chunk or in an R script, insert a timestamp with `ts` + `shift` + `tab`.

First write `ts`:

```{r, eval=FALSE}
ts
```

Then press `shift` + `tab`:

```{r}
# Wed Nov 11 10:42:54 2020 ------------------------------

```

# Making comparisons by groups? - **[Emi Tanaka](https://github.com/emitanaka)**

* Check it out with a plot! 
* Make multiples/facets if there's too many observations per group!
* Easily done with `ggplot2`!

```{r no-shadow}
ggplot(ChickWeight, aes(Time, weight, group = Chick)) +
  geom_line(aes(color = Diet)) +
  facet_wrap(~Diet) +
  theme_bw() +
  labs(y = "Weight") +
  scale_color_viridis_d()
```

* But make it even easier to compare by **adding a shadow of all data for each facet**.
* Only takes one extra line in `ggplot2`. Replace data with the variable you are facetting with dropped and change color to gray like in the second line of code below. 

```{r with-shadow}
ggplot(ChickWeight, aes(Time, weight, group = Chick)) +
  geom_line(data = select(ChickWeight, -Diet), color = "gray") + 
  geom_line(aes(color = Diet)) +
  facet_wrap(~Diet) +
  theme_bw() +
  labs(y = "Weight") +
  scale_color_viridis_d()

```

# R Tips for (Sport and Non-Sport) Scientists and/ or those who simply love using R! - **[Alice Sweeting](http://sportstatisticsrsweet.rbind.io/)**

```{r global_options, R.options=knitr::opts_chunk$set(warning=FALSE, message=FALSE)}
```


Below are some tips and tricks that I have picked up along my R journey, that may (or may not!) be useful for those working with sport science data, or any data, in R. I hope you find the tips useful!


![](https://media.giphy.com/media/TEdAFYzzDEq1vNi2Qf/source.gif)



<br>

## Visualising Netball Data

If you are interested in working with some real life netball data, you can see a slidedeck that I put together of how to import, tidy and analyse the data [here.](https://sportstatisticsrsweet.github.io/RNetball/Slides)

## Working with Files from Drobpbox

Sometimes, someone will share some data with us via Dropbox. Often these files can be **really** big and it is annoying to download a local copy of these files on your machine. If you have been sent a link to view these files, the following may be useful so you can load them directly into R, without having to save the file locally on your machine first.

1. Get "Share (or Copy) Dropbox Link" from file.
  This might look like: https://www.dropbox.com/s/sometexthere/NameOfFile.csv?dl=0
2. Change dl=0 to dl=1 at end of link.
  This might look like: https://www.dropbox.com/s/sometexthere/NameOfFile.csv?dl=1
3. Run the following code
```{r eval=FALSE}
# Install the {vroom} package
install.packages(vroom)
# Load the package from your library
library(vroom)
# Load your data directly into R!
data <- vroom("https://www.dropbox.com/s/sometexthere/NameOfFile.csv?dl=1")
```

## Quickly importing Athlete Management System (AMS) Data into R

Use R to quickly import AMS your data and not even open a web browser! To do this, please follow the steps below. Note – I am using Smartabase as an example here because it is familiar to me. 

1. Save a report of your data within Smartabase. For example, save a report that contains all historical athlete wellness data.
2. Open R and use the code below to import your data. Note, you will need to add your own username/ password/ URL details that are specific to your team.

```{r eval=FALSE}
# Load required packages
library(rvest)
library(plyr)
library(dplyr)
library(qdap)
# Connect to a report that you have generated via Smartabase
WellnessDataURL <- html_session("https://username:password@my2.smartabase.com/yourteamsname/live?report=WellnessData&updategroup=true")
# Read in data
WellnessData <- read_html(WellnessDataURL)
# Identify the table
WellnessDataTable <- html_nodes(WellnessData, "table")
# Collect only table data
WellnessDataTable1 <- html_table(WellnessDataTable[1], fill = TRUE)
# Make data.frame
HistoricalWellnessData <- as.data.frame(WellnessDataTable1)
# Clean Environment
rm(list = grep("^HistoricalWellnessData", ls(), value = TRUE, invert = TRUE))
```

Now your AMS data is in a neat data.frame and ready for any further statistical analysis or visualisation using R, without needing to open a web browser!

<br>

![

# Making a cartogram of the 2020 Victorian COVID-19 outbreak - **[Dianne Cook](http://dicook.org)**

```{r libraries, echo=FALSE}
library(tidyverse)
library(lubridate)
library(forcats)
library(ozmaps)
library(sf)
library(cartogram)
library(ggthemes)
library(plotly)
library(sugarbag)
```

In Melbourne we have been in a strict lockdown since July. Each week we get our hopes up that restrictions might be eased, and once again these hopes are dashed by the announcement Sunday Oct 25, keeping the restrictions a little longer because of another outbreak in the northwest of the city. The data we have collected here are the case counts by Victorian local government area (LGA) since the beginning of July. We will examine the spatiotemporal distribution of these counts. 

Working with spatial data is always painful! It almost always requires some **ugly** code. Part of the reason for the difficulty is the use of special data objects, that describe maps. There are several different choices, and some packages and tools use one, and others use another, so not all tools work together. The `sf` package is a recent endeavour that helps enormously, but some tools still use other forms, and when you run into errors this might be the reason - it can be hard to tell. Another reason is that map objects can be very large, which makes sense for accurate mapping, but for data analysis and visualisation, we'd rather have smaller, even if slightly inaccurate, spatial objects. It can be helpful to thin out map data before doing further analysis - you need special tools for this, eg `mapshapr`. We don't need this for the exercises here. Another problem commonly encountered is that there are numerous coordinate systems, and types of projections of the 3D globe into a 2D canvas. We have become accustomed to lat/long but like time its an awkward scale to compute on because a translation from E/W and N/S to positive and negative values is needed. More commonly a Universal Transverse Mercator (UTM) is the standard but its far less intuitive to use.  

The code for all the analysis is provided for you. We recommend that you run the code in steps to see what it is doing, why the mutating and text manipulations are necessary. Talk about the code with each other to help you understand it. 


## Getting the data

### COVID-19 counts

COVID-19 data by LGA is available from https://www.covid19data.com.au/victoria. You should find that some variables are type `chr` because "null" has been used to code entries on some days. This needs fixing, and also missings should be converted to 0, with the rationale being that if the value is missing it most likely means there were no cases.


```{r}
# Read the data
# Replace null with 0, for three LGAs
# Convert to long form to join with polygons
# Make the date variables a proper date
# Set NAs to 0, this is a reasonable assumption
covid <- read_csv("https://raw.githubusercontent.com/numbats/eda/master/data/melb_lga_covid.csv") %>%
  mutate(Buloke = as.numeric(ifelse(Buloke == "null", "0", Buloke))) %>%
  mutate(Hindmarsh = as.numeric(ifelse(Hindmarsh == "null", "0", Hindmarsh))) %>%
   mutate(Towong = as.numeric(ifelse(Towong == "null", "0", Towong))) %>%
  pivot_longer(cols = Alpine:Yarriambiack, names_to="NAME", values_to="cases") %>%
  mutate(Date = ydm(paste0("2020/",Date))) %>%
  mutate(cases=replace_na(cases, 0))

# Case counts are cumulative, so take lags to get daily case counts
covid <- covid %>%
  group_by(NAME) %>%
  mutate(new_cases = cases - dplyr::lag(cases))

# Filter to final day, which is cumulative count
covid_cumul <- covid %>% 
  filter(Date == max(Date)) 
```

### Spatial polygons

Now let's get polygon data of Victorian LGAs from the `ozmaps` package. We need to fix some names of LGAs because there are duplicated LGA names, and there is one mismatch in names from the COVID data and the ozmaps data (Colac Otway). If the COVID data had been provided with a unique LGA code it would have helped in merging with the polygon data.

```{r vic_lga}
# Read the LGA data from ozmaps package. 
# This has LGAs for all of Australia. 
# Need to filter out Victoria LGAs, avoiding LGAs 
# from other states with same name, and make the names
# match covid data names. The regex equation is
# removing () state and LGA type text strings
# Good reference: https://r-spatial.github.io/sf/articles/sf1.html
data("abs_lga")
vic_lga <- abs_lga %>%
  mutate(NAME = ifelse(NAME == "Latrobe (M) (Tas.)", "LatrobeM", NAME)) %>%
  mutate(NAME = ifelse(NAME == "Kingston (DC) (SA)", "KingstonSA", NAME)) %>%
  mutate(NAME = ifelse(NAME == "Bayside (A)", "BaysideA", NAME)) %>% 
  mutate(NAME = str_replace(NAME, " \\(.+\\)", "")) %>%
  mutate(NAME = ifelse(NAME == "Colac-Otway", "Colac Otway", NAME)) 
vic_lga <- st_transform(vic_lga, 3395) 
# 3395 is EPSG CRS, equiv to WGS84 mercator, 
# see https://spatialreference.org/ref/epsg/?page=28
# cartogram() needs this to be set
```

### Choropleth map

A choropleth map is made from filling the colour of polygons. 

```{r covid-choropleth, fig.width=10, fig.height=10, out.width="100%"}
# Join covid data to polygon data, remove LGAs with 
# missing values which should leave just Vic LGAs
vic_lga_covid <- vic_lga %>%
  left_join(covid_cumul, by="NAME") %>%
  filter(!is.na(cases))

# Make choropleth map, with appropriate colour palette
choropleth <- ggplot(vic_lga_covid) + 
  geom_sf(aes(fill = cases, label=NAME), colour="white") + 
  scale_fill_distiller("Cases", palette = "YlOrRd",
                       direction=1) + 
  theme_map() +
  theme(legend.position="bottom")
choropleth
#ggplotly(choropleth) # Interactive map
```

## Population-transformed cartogram

### Get population data

 The file [`VIF2019_Population_Service_Ages_LGA_2036.xlsx`](https://github.com/numbats/eda/blob/master/data/VIF2019_Population_Service_Ages_LGA_2036.xlsx)  has been extracted from the [Vic Gov web site](https://www.planning.vic.gov.au/land-use-and-population-research/victoria-in-future/tab-pages/victoria-in-future-data-tables). It is a complicated `xlsx` file, with the data in sheet 3, and starting 13 rows down. The `readxl` package is handy here to extract the population data needed. The code below has extracted only the data needed.
 
```{r population}
pop <- tibble(NAME = c("Alpine","Ararat","Ballarat","Banyule","Bass Coast","Baw Baw","Bayside","Benalla","Boroondara","Brimbank","Buloke","Campaspe","Cardinia","Casey","Central Goldfields","Colac Otway","Corangamite","Darebin","East Gippsland","Frankston","Gannawarra","Glen Eira","Glenelg","Golden Plains","Greater Bendigo","Greater Dandenong","Greater Geelong","Greater Shepparton","Hepburn","Hindmarsh","Hobsons Bay","Horsham","Hume","Indigo","Kingston","Knox","Latrobe","Loddon","Macedon Ranges","Manningham","Mansfield","Maribyrnong","Maroondah","Melbourne","Melton","Mildura","Mitchell","Moira","Monash","Moonee Valley","Moorabool","Moreland","Mornington Peninsula","Mount Alexander","Moyne","Murrindindi","Nillumbik","Northern Grampians","Port Phillip","Pyrenees","Queenscliffe","South Gippsland","Southern Grampians","Stonnington","Strathbogie","Surf Coast","Swan Hill","Towong","Wangaratta","Warrnambool","Wellington","West Wimmera","Whitehorse","Whittlesea","Wodonga","Wyndham","Yarra","Yarra Ranges","Yarriambiack"), 
pop = c(12578,11746.43,103500.3,127447,33464.85,49296.21,102912,13981.3,177276,204190,6284,37596.09,97572.66,312789,13085.32,21362.81,16241,155126,45598.55,139502,10567.15,148583,19758.61,22016,112270.9,160222,239529.9,65071.32,15526.87,5787.223,93445.04,19884.51,207038,16165.73,158937.6,160353.5,74622.36,7559.041,47479.75,122570.7,8674.158,86942,114799.3,146097.1,141422.1,54658,41794.85,29486,192625,122871.1,32668.76,172289.5,161528,19093.7,16738.47,14052.73,64174,11570.29,108627.2,7315.398,2927.166,29120.95,16122.74,111003,10357.01,30465.01,20895.86,6045.765,28592,34243.1,43531.44,3932.907,169641.6,207058,40100,227008,92898.52,155227.4,6742.772))

vic_lga_covid <- vic_lga_covid %>%
  left_join(pop, by="NAME") 

# Compute additional statistics
vic_lga_covid <- vic_lga_covid %>%
  group_by(NAME) %>%
  mutate(cases_per10k = max(cases/pop*10000, 0)) %>%
  ungroup()
```

### Make the cartogram

```{r vic-cartogram, fig.width=10, fig.height=10, out.width="100%"}
vic_lga_covid_carto <- cartogram_cont(vic_lga_covid, "pop")
# This st_cast() is necessary to get plotly to work
vic_lga_covid_carto <- st_cast(vic_lga_covid_carto, "MULTIPOLYGON") 

cartgram <- ggplot(vic_lga_covid_carto) + 
  geom_sf(aes(fill = cases_per10k, label=NAME), colour="white") + 
  scale_fill_distiller("Cases", palette = "YlOrRd",
                       direction=1) + 
  theme_map() +
  theme(legend.position="bottom") 
cartgram 
#ggplotly(cartgram) # Interactive cartogram
```

## Lastly, a hexagon tile map

The hexagon tile map makes tiled hexagons representing the LGAs. You can read more about it in the documentation for the `sugarbag` package at https://srkobakian.github.io/sugarbag/.

```{r hextile, fig.width=10, fig.height=10, out.width="100%"}
# Spatial coordinates need to be in long/lat
vlc_latlong <- st_transform(vic_lga_covid, crs = "+proj=longlat +datum=WGS84")

# Placement of hexmaps depends on position relative to
# Melbourne central
data(capital_cities)
vic_lga_hexmap <- create_hexmap(
  shp = vlc_latlong,
  sf_id = "NAME",
  focal_points = capital_cities, verbose = TRUE)
# This shows the centroids of the hexagons
# ggplot(vic_lga_hexmap, aes(x=hex_long, y=hex_lat)) +
#  geom_point()

# Hexagons are made with the `fortify_hexagon` function
vic_lga_covid_hexmap <- vic_lga_hexmap %>%
  fortify_hexagon(sf_id = "NAME", hex_size = 0.1869) %>%
  left_join(covid_cumul, by="NAME") %>%
  filter(!is.na(cases)) %>%
  left_join(pop, by="NAME") %>%
  group_by(NAME) %>%
  mutate(cases_per10k = max(cases/pop*10000, 0)) %>%
  ungroup()

hexmap <- ggplot() +
  geom_sf(data=vlc_latlong, 
          fill = "grey90", colour = "white", size=0.1) +
  geom_polygon(data=vic_lga_covid_hexmap, 
               aes(x=long, y=lat, group=hex_id, 
                   fill = cases_per10k, 
                   colour = cases_per10k, 
                   label=NAME), 
               size=0.2) +
  scale_fill_distiller("Cases", palette = "YlOrRd",
                       direction=1) +
  scale_colour_distiller("Cases", palette = "YlOrRd",
                       direction=1) +
  theme_map() +
  theme(legend.position="bottom", aspect.ratio=0.7)
hexmap
# ggplotly(hexmap)
```





